{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > tts_models/multilingual/multi-dataset/xtts_v2 is already downloaded.\n",
      " > Using model: xtts\n",
      "Imports done\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from TTS.api import TTS\n",
    "\n",
    "# Get device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# List available ðŸ¸TTS models\n",
    "# print(TTS().list_models())\n",
    "\n",
    "# Init TTS\n",
    "tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\").to(device)\n",
    "\n",
    "print(\"Imports done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TTS on CSV function\n",
    "\n",
    "def tts_csv(csv_path, metadata_dir):\n",
    "    # Load CSV\n",
    "    df = pd.read_csv(csv_path, sep=\",\")\n",
    "    df.columns = df.columns.str.strip()\n",
    "\n",
    "    # Create output folder\n",
    "    output_folder = r\"D:\\kelse\\03_Repositories\\coquiTTS\\notebooks\\kelseyNotebooks\\soundLib\\live\"\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Iterate through files in the audio folder\n",
    "    for filename in os.listdir(metadata_dir):\n",
    "        if filename.endswith(\".wav\"):  \n",
    "            speaker_wav = os.path.join(metadata_dir, filename)\n",
    "\n",
    "            filename_base = os.path.splitext(filename)[0]\n",
    "\n",
    "            # Check if the filename is in the CSV, with or without the extension\n",
    "            if filename_base in df['File'].str.replace(\".wav\", \"\").values:\n",
    "                # Get the corresponding text from the CSV based on the filename\n",
    "                norm_text = df.loc[df['File'].str.replace(\".wav\", \"\") == filename_base, 'Normalized Transcription'].values[0]\n",
    "\n",
    "                # Run TTS\n",
    "                wav = tts.tts(text=norm_text, speaker_wav=speaker_wav, language=\"en\")\n",
    "                \n",
    "                # Construct the output file path for TTS audio\n",
    "                output_file_path = os.path.join(output_folder, os.path.splitext(filename)[0] + \"_TTS_output.wav\")\n",
    "            \n",
    "                # Save TTS audio to file\n",
    "                tts.tts_to_file(text=norm_text, speaker_wav=speaker_wav, language=\"en\", file_path=output_file_path)\n",
    "\n",
    "\n",
    "            if os.path.splitext(filename)[0] in df[df.columns[0]].str.replace(\".wav\", \"\").values:\n",
    "                # Get the corresponding text from the CSV based on the filename\n",
    "                norm_text = df.loc[df[df.columns[0]].str.replace(\".wav\", \"\") == os.path.splitext(filename)[0], df.columns[2]].values[0]\n",
    "\n",
    "                # Run TTS\n",
    "                wav = tts.tts(text=norm_text, speaker_wav=speaker_wav, language=\"en\")\n",
    "                \n",
    "                # Construct the output file path for TTS audio\n",
    "                output_file_path = os.path.join(output_folder, os.path.splitext(filename)[0] + \"_TTS_output.wav\")\n",
    "            \n",
    "                # Save TTS audio to file\n",
    "                tts.tts_to_file(text=norm_text, speaker_wav=speaker_wav, language=\"en\", file_path=output_file_path)\n",
    "            else:\n",
    "                print(f\"Filename {filename} not found in the CSV.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kelse\\AppData\\Local\\Temp\\ipykernel_10428\\2707478043.py:21: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  if filename_base in df['File'].str.replace(\".wav\", \"\").values:\n",
      "C:\\Users\\kelse\\AppData\\Local\\Temp\\ipykernel_10428\\2707478043.py:23: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  norm_text = df.loc[df['File'].str.replace(\".wav\", \"\") == filename_base, 'Normalized Transcription'].values[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text splitted to sentences.\n",
      "['Oh']\n",
      " > Processing time: 0.728877067565918\n",
      " > Real-time factor: 0.6032935187623307\n",
      " > Text splitted to sentences.\n",
      "['Oh']\n",
      " > Processing time: 0.6689326763153076\n",
      " > Real-time factor: 0.4759281592911891\n",
      " > Text splitted to sentences.\n",
      "['Oh']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kelse\\AppData\\Local\\Temp\\ipykernel_10428\\2707478043.py:35: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  if os.path.splitext(filename)[0] in df[df.columns[0]].str.replace(\".wav\", \"\").values:\n",
      "C:\\Users\\kelse\\AppData\\Local\\Temp\\ipykernel_10428\\2707478043.py:37: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  norm_text = df.loc[df[df.columns[0]].str.replace(\".wav\", \"\") == os.path.splitext(filename)[0], df.columns[2]].values[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Processing time: 0.4728577136993408\n",
      " > Real-time factor: 0.37689822827756164\n",
      " > Text splitted to sentences.\n",
      "['Oh']\n",
      " > Processing time: 0.42374324798583984\n",
      " > Real-time factor: 0.3507334316099012\n",
      " > Text splitted to sentences.\n",
      "['Oh, city, city.']\n",
      " > Processing time: 1.0713894367218018\n",
      " > Real-time factor: 0.4612647820937935\n",
      " > Text splitted to sentences.\n",
      "['Oh, city, city.']\n",
      " > Processing time: 1.8252599239349365\n",
      " > Real-time factor: 0.45826859768133255\n",
      " > Text splitted to sentences.\n",
      "['Oh, city, city.']\n",
      " > Processing time: 1.819042682647705\n",
      " > Real-time factor: 0.6001599705587428\n",
      " > Text splitted to sentences.\n",
      "['Oh, city, city.']\n",
      " > Processing time: 1.2361154556274414\n",
      " > Real-time factor: 0.47097639266977265\n",
      " > Text splitted to sentences.\n",
      "['ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„ì•„']\n",
      "[!] Warning: The text length exceeds the character limit of 250 for language 'en', this might cause truncated audio.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": " â— XTTS can only generate text with a maximum of 400 tokens.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 10\u001b[0m\n\u001b[0;32m      6\u001b[0m metadata_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mkelse\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m03_Repositories\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mITMTS\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mhandy_base_scripts\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mHBS_recordings\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Run TTS on CSV\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[43mtts_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 26\u001b[0m, in \u001b[0;36mtts_csv\u001b[1;34m(csv_path, metadata_dir)\u001b[0m\n\u001b[0;32m     23\u001b[0m norm_text \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mloc[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFile\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m filename_base, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNormalized Transcription\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Run TTS\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m wav \u001b[38;5;241m=\u001b[39m \u001b[43mtts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspeaker_wav\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspeaker_wav\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43men\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Construct the output file path for TTS audio\u001b[39;00m\n\u001b[0;32m     29\u001b[0m output_file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_folder, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(filename)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_TTS_output.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\cuda\\lib\\site-packages\\TTS\\api.py:276\u001b[0m, in \u001b[0;36mTTS.tts\u001b[1;34m(self, text, speaker, language, speaker_wav, emotion, speed, split_sentences, **kwargs)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Convert text to speech.\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \n\u001b[0;32m    250\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;124;03m        Additional arguments for the model.\u001b[39;00m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_arguments(\n\u001b[0;32m    274\u001b[0m     speaker\u001b[38;5;241m=\u001b[39mspeaker, language\u001b[38;5;241m=\u001b[39mlanguage, speaker_wav\u001b[38;5;241m=\u001b[39mspeaker_wav, emotion\u001b[38;5;241m=\u001b[39memotion, speed\u001b[38;5;241m=\u001b[39mspeed, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    275\u001b[0m )\n\u001b[1;32m--> 276\u001b[0m wav \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msynthesizer\u001b[38;5;241m.\u001b[39mtts(\n\u001b[0;32m    277\u001b[0m     text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[0;32m    278\u001b[0m     speaker_name\u001b[38;5;241m=\u001b[39mspeaker,\n\u001b[0;32m    279\u001b[0m     language_name\u001b[38;5;241m=\u001b[39mlanguage,\n\u001b[0;32m    280\u001b[0m     speaker_wav\u001b[38;5;241m=\u001b[39mspeaker_wav,\n\u001b[0;32m    281\u001b[0m     reference_wav\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    282\u001b[0m     style_wav\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    283\u001b[0m     style_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    284\u001b[0m     reference_speaker_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    285\u001b[0m     split_sentences\u001b[38;5;241m=\u001b[39msplit_sentences,\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    287\u001b[0m )\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wav\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\cuda\\lib\\site-packages\\TTS\\utils\\synthesizer.py:386\u001b[0m, in \u001b[0;36mSynthesizer.tts\u001b[1;34m(self, text, speaker_name, language_name, speaker_wav, style_wav, style_text, reference_wav, reference_speaker_name, split_sentences, **kwargs)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sen \u001b[38;5;129;01min\u001b[39;00m sens:\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtts_model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msynthesize\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 386\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtts_model\u001b[38;5;241m.\u001b[39msynthesize(\n\u001b[0;32m    387\u001b[0m             text\u001b[38;5;241m=\u001b[39msen,\n\u001b[0;32m    388\u001b[0m             config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtts_config,\n\u001b[0;32m    389\u001b[0m             speaker_id\u001b[38;5;241m=\u001b[39mspeaker_name,\n\u001b[0;32m    390\u001b[0m             voice_dirs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvoice_dir,\n\u001b[0;32m    391\u001b[0m             d_vector\u001b[38;5;241m=\u001b[39mspeaker_embedding,\n\u001b[0;32m    392\u001b[0m             speaker_wav\u001b[38;5;241m=\u001b[39mspeaker_wav,\n\u001b[0;32m    393\u001b[0m             language\u001b[38;5;241m=\u001b[39mlanguage_name,\n\u001b[0;32m    394\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    395\u001b[0m         )\n\u001b[0;32m    396\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    397\u001b[0m         \u001b[38;5;66;03m# synthesize voice\u001b[39;00m\n\u001b[0;32m    398\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m synthesis(\n\u001b[0;32m    399\u001b[0m             model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtts_model,\n\u001b[0;32m    400\u001b[0m             text\u001b[38;5;241m=\u001b[39msen,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    408\u001b[0m             language_id\u001b[38;5;241m=\u001b[39mlanguage_id,\n\u001b[0;32m    409\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\cuda\\lib\\site-packages\\TTS\\tts\\models\\xtts.py:419\u001b[0m, in \u001b[0;36mXtts.synthesize\u001b[1;34m(self, text, config, speaker_wav, language, speaker_id, **kwargs)\u001b[0m\n\u001b[0;32m    412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minference(text, language, gpt_cond_latent, speaker_embedding, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msettings)\n\u001b[0;32m    413\u001b[0m settings\u001b[38;5;241m.\u001b[39mupdate({\n\u001b[0;32m    414\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt_cond_len\u001b[39m\u001b[38;5;124m\"\u001b[39m: config\u001b[38;5;241m.\u001b[39mgpt_cond_len,\n\u001b[0;32m    415\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt_cond_chunk_len\u001b[39m\u001b[38;5;124m\"\u001b[39m: config\u001b[38;5;241m.\u001b[39mgpt_cond_chunk_len,\n\u001b[0;32m    416\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_ref_len\u001b[39m\u001b[38;5;124m\"\u001b[39m: config\u001b[38;5;241m.\u001b[39mmax_ref_len,\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msound_norm_refs\u001b[39m\u001b[38;5;124m\"\u001b[39m: config\u001b[38;5;241m.\u001b[39msound_norm_refs,\n\u001b[0;32m    418\u001b[0m })\n\u001b[1;32m--> 419\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfull_inference(text, speaker_wav, language, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msettings)\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\cuda\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\cuda\\lib\\site-packages\\TTS\\tts\\models\\xtts.py:488\u001b[0m, in \u001b[0;36mXtts.full_inference\u001b[1;34m(self, text, ref_audio_path, language, temperature, length_penalty, repetition_penalty, top_k, top_p, do_sample, gpt_cond_len, gpt_cond_chunk_len, max_ref_len, sound_norm_refs, **hf_generate_kwargs)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;124;03mThis function produces an audio clip of the given text being spoken with the given reference voice.\u001b[39;00m\n\u001b[0;32m    443\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;124;03m    Sample rate is 24kHz.\u001b[39;00m\n\u001b[0;32m    479\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    480\u001b[0m (gpt_cond_latent, speaker_embedding) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_conditioning_latents(\n\u001b[0;32m    481\u001b[0m     audio_path\u001b[38;5;241m=\u001b[39mref_audio_path,\n\u001b[0;32m    482\u001b[0m     gpt_cond_len\u001b[38;5;241m=\u001b[39mgpt_cond_len,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m     sound_norm_refs\u001b[38;5;241m=\u001b[39msound_norm_refs,\n\u001b[0;32m    486\u001b[0m )\n\u001b[1;32m--> 488\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minference(\n\u001b[0;32m    489\u001b[0m     text,\n\u001b[0;32m    490\u001b[0m     language,\n\u001b[0;32m    491\u001b[0m     gpt_cond_latent,\n\u001b[0;32m    492\u001b[0m     speaker_embedding,\n\u001b[0;32m    493\u001b[0m     temperature\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[0;32m    494\u001b[0m     length_penalty\u001b[38;5;241m=\u001b[39mlength_penalty,\n\u001b[0;32m    495\u001b[0m     repetition_penalty\u001b[38;5;241m=\u001b[39mrepetition_penalty,\n\u001b[0;32m    496\u001b[0m     top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[0;32m    497\u001b[0m     top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[0;32m    498\u001b[0m     do_sample\u001b[38;5;241m=\u001b[39mdo_sample,\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhf_generate_kwargs,\n\u001b[0;32m    500\u001b[0m )\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\cuda\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\cuda\\lib\\site-packages\\TTS\\tts\\models\\xtts.py:537\u001b[0m, in \u001b[0;36mXtts.inference\u001b[1;34m(self, text, language, gpt_cond_latent, speaker_embedding, temperature, length_penalty, repetition_penalty, top_k, top_p, do_sample, num_beams, speed, enable_text_splitting, **hf_generate_kwargs)\u001b[0m\n\u001b[0;32m    533\u001b[0m sent \u001b[38;5;241m=\u001b[39m sent\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mlower()\n\u001b[0;32m    534\u001b[0m text_tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mIntTensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mencode(sent, lang\u001b[38;5;241m=\u001b[39mlanguage))\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m--> 537\u001b[0m     text_tokens\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgpt_max_text_tokens\n\u001b[0;32m    538\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m â— XTTS can only generate text with a maximum of 400 tokens.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    540\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m    541\u001b[0m     gpt_codes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgpt\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m    542\u001b[0m         cond_latents\u001b[38;5;241m=\u001b[39mgpt_cond_latent,\n\u001b[0;32m    543\u001b[0m         text_inputs\u001b[38;5;241m=\u001b[39mtext_tokens,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhf_generate_kwargs,\n\u001b[0;32m    555\u001b[0m     )\n",
      "\u001b[1;31mAssertionError\u001b[0m:  â— XTTS can only generate text with a maximum of 400 tokens."
     ]
    }
   ],
   "source": [
    "# Set your CSV file path here\n",
    "\n",
    "csv_path = r\"D:\\kelse\\03_Repositories\\ITMTS\\kelseyASR\\output_csv_path\\2024-02-25_runhbs_whisperAI_transcriptions.csv\"\n",
    "\n",
    "# Directory where metadata files are stored\n",
    "metadata_dir = r\"D:\\kelse\\03_Repositories\\ITMTS\\handy_base_scripts\\HBS_recordings\"\n",
    "\n",
    "\n",
    "# Run TTS on CSV\n",
    "tts_csv(csv_path, metadata_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TTS on CSV done\n"
     ]
    }
   ],
   "source": [
    "print(\"TTS on CSV done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
